{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic modeling for restaurant success/failure based on current attributes\n",
    "We'll create 3 dataframes for modeling: \n",
    "\n",
    "1) Only data that is available when restaurant opens (excluding any review data)\n",
    "\n",
    "2) DF 1 + review counts from first 4 and 8 weeks\n",
    "\n",
    "3) DF 2 + Word2Vec data (limits us to ~900 restaurants that had enough reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in neigbhor and census features.\n",
    "# Excludes restaurants that opened after 2015 (can't meet our def of success)\n",
    "rest_df = pd.read_csv('./data/business_neighbors_census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the success data\n",
    "success_df = pd.read_pickle('./data/features_df_3mo.pi')\n",
    "success_df['successful'] = ((success_df['age']>=4.5) &\n",
    "                            (success_df['is_open']) &\n",
    "                            (success_df['stars']>=3.5) &\n",
    "                            (success_df['review_count'] >= 20))\n",
    "cols_to_merge = success_df.columns.difference(rest_df.columns)\n",
    "cols_to_merge = np.append(cols_to_merge.values, 'business_id')\n",
    "# Merge and get open year\n",
    "rest_df = rest_df.merge(success_df[cols_to_merge], on='business_id')\n",
    "for col in ['num_in_4_weeks', 'num_in_8_weeks']:\n",
    "    rest_df[col] = rest_df[col].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in Ryan's restaurant attributes\n",
    "att_df = pd.read_pickle('./data/restaurants_imputed.pkl.bz2')\n",
    "att_df = att_df.loc[:, att_df.columns.str.startswith(r'biz')]\n",
    "rest_df = rest_df.merge(att_df, on='business_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_3mo_df = rest_df.copy()\n",
    "rest_df = rest_df.drop(columns=['first_4_week_review', 'first_8_week_review',\n",
    "       'num_in_4_weeks', 'num_in_8_weeks', 'average_review_length', 'review_length_variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in text data\n",
    "words_df = pd.read_pickle('./data/word2vec_keywordEmbeddings.pkl').transpose()\n",
    "words_df.columns = ['w2v_{}'.format(c) for c in words_df.columns]\n",
    "words_df['business_id'] = words_df.index.values\n",
    "rest_3mo_words_df = rest_3mo_df.merge(words_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our 3 data frames:\n",
    "print('Base DF {}'.format(rest_df.shape))\n",
    "print('Info up to 3 monghs {}'.format(rest_3mo_df.shape))\n",
    "print('Text Info (3 months, >= 10 reviews) {}'.format(rest_3mo_words_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for training, eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for cross validation testing and var imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_cross_val(pipeline, X, y, print_results=False):\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_weighted')\n",
    "    if print_results:\n",
    "        print('Cross Val Scores: {}'.format(cv_scores))\n",
    "        print('CV Mean F1: {}'.format(np.mean(cv_scores)))\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "def pipeline_final_test(pipeline, X_train, y_train, X_test, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    return f1_test\n",
    "\n",
    "def all_models_cv_test(df_dict, final_test=False):\n",
    "    out_df = pd.DataFrame(index=list(df_dict.keys()),columns=['RandomForest', 'SVM', 'LogReg', 'NaiveBayes'])\n",
    "    \n",
    "    ### Define pipelines\n",
    "    scaler = StandardScaler()\n",
    "    # LR\n",
    "    lr_c = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    lr_pipeline = Pipeline([('scale', scaler), ('clf', lr_c)])\n",
    "    # NB\n",
    "    nb_c = GaussianNB()\n",
    "    nb_pipeline = Pipeline([('clf', nb_c)])\n",
    "    # RF\n",
    "    rf_c = RandomForestClassifier(n_estimators=200, random_state=25, min_samples_leaf=1)\n",
    "    rf_pipeline = Pipeline([('clf', rf_c)])\n",
    "    # SVM\n",
    "    svm_c = SVC(gamma='auto', kernel='rbf')\n",
    "    svm_pipeline = Pipeline([('scale', scaler), ('clf', svm_c)])    \n",
    "    \n",
    "    # Loop over our datasets\n",
    "    for df_name in df_dict.keys():\n",
    "        df = df_dict[df_name]     \n",
    "        \n",
    "        ### Split\n",
    "        # Set input cols to exclude business ID and the actual success statistics. \n",
    "        # Can be modified more to test different variable combinations\n",
    "        non_input_cols = np.array(['business_id', 'is_open', 'successful', 'stars', \n",
    "                                   'review_count', 'age', 'first_review'])\n",
    "        input_cols = df.columns[np.isin(df.columns.values, non_input_cols, invert=True)]\n",
    "        X = df[input_cols].values\n",
    "        y = df['successful'].values\n",
    "\n",
    "        # Hold out test set for final test. Use cross val for development\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=25)\n",
    "\n",
    "        if not final_test:\n",
    "            out_df.loc[df_name, 'LogReg'] = [pipeline_cross_val(\n",
    "                lr_pipeline, X_train, y_train)]\n",
    "            out_df.loc[df_name, 'RandomForest'] = [pipeline_cross_val(\n",
    "                rf_pipeline, X_train, y_train)]\n",
    "            out_df.loc[df_name, 'SVM'] = [pipeline_cross_val(\n",
    "                svm_pipeline, X_train, y_train)]\n",
    "            out_df.loc[df_name, 'NaiveBayes'] = [pipeline_cross_val(\n",
    "                nb_pipeline, X_train, y_train)]\n",
    "        else:\n",
    "            out_df.loc[df_name, 'LogReg'] = pipeline_final_test(\n",
    "                lr_pipeline, X_train, y_train, X_test, y_test)\n",
    "            out_df.loc[df_name, 'RandomForest'] = pipeline_final_test(\n",
    "                rf_pipeline, X_train, y_train, X_test, y_test)\n",
    "            out_df.loc[df_name, 'SVM'] = pipeline_final_test(\n",
    "                svm_pipeline, X_train, y_train, X_test, y_test)\n",
    "            out_df.loc[df_name, 'NaiveBayes'] = pipeline_final_test(\n",
    "                nb_pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_imp_plot(var_imps, input_col_names, topn=15, title=None):\n",
    "    indices = np.argsort(var_imps)[::-1]\n",
    "    indices = indices[0:topn+1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    \n",
    "    input_col_sorted = input_col_names.values[indices]\n",
    "    for i in range(indices.shape[0]):\n",
    "        print('{}: {}'.format(input_col_sorted[i], round(var_imps[indices[i]], 3)))\n",
    "        \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    if title==None:\n",
    "        plt.title(\"Feature importances\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.barh(np.flip(input_col_sorted), np.flip(var_imps[indices]),\n",
    "           color=\"r\", align=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def var_imp_plot(var_imps, input_col_names, topn=10, title=None):\n",
    "    plt.figure(figsize=(8, 8), facecolor='white')\n",
    "    indices = np.argsort(var_imps)[::-1]\n",
    "    indices = indices[0:topn+1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    \n",
    "    input_col_sorted = input_col_names.values[indices]\n",
    "    for i in range(indices.shape[0]):\n",
    "        print('{}: {}'.format(input_col_sorted[i], round(var_imps[indices[i]], 3)))\n",
    "        \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    if title==None:\n",
    "        plt.title(\"Feature importances\")\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel('Feature Importance', fontsize=18)\n",
    "        \n",
    "    plt.barh(np.flip(input_col_sorted), np.flip(var_imps[indices]),\n",
    "           color=\"r\", align=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/CrossVal/VarImp, all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_models_cv_test({'Base':rest_df,\n",
    "                             'First 3 Months':rest_3mo_df,\n",
    "                             'First 3 Months + Keywords':rest_3mo_words_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_models_cv_test({'Base':rest_df, 'First 3 Months':rest_3mo_df, 'First 3 Months + Keywords':rest_3mo_words_df},\n",
    "                             final_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor='white', figsize=(10, 7))\n",
    "final_df.loc[['Base', 'First 3 Months', 'First 3 Months + Keywords']\n",
    "            ].plot(kind='bar', ax=ax)\n",
    "plt.ylabel('F1 Score (Weighted)', fontsize=18)\n",
    "plt.xticks(labels=['Initial Data', 'First 3 Months', '3 Months + Keywords'], \n",
    "           ticks=[0,1,2], fontsize=16, rotation=30, ha='right')\n",
    "plt.xlim(-0.5, 3.4)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = rest_3mo_df.copy() # Set which df you're working with here\n",
    "# Set input cols to exclude business ID and the actual success statistics. \n",
    "# Can be modified more to test different variable combinations\n",
    "non_input_cols = np.array(['business_id', 'is_open', 'successful', 'stars', \n",
    "                           'review_count', 'age', 'first_review', 'average_review_length'])\n",
    "input_cols = target_df.columns[np.isin(target_df.columns.values, non_input_cols, invert=True)]\n",
    "X = target_df[input_cols].values\n",
    "y = target_df['successful'].values\n",
    "# Hold out test set for final test. Use cross val for development\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "lr_c = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr_pipeline = Pipeline([('scale', scaler), ('clf', lr_c)])\n",
    "pipeline_cross_val(lr_pipeline, X_train, y_train, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_varimp = lr_pipeline.steps[1][1].coef_\n",
    "var_imp_plot(lr_varimp[0], input_cols, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_c = RandomForestClassifier(n_estimators=200, random_state=25, min_samples_leaf=1)\n",
    "rf_pipeline = Pipeline([('clf', rf_c)])\n",
    "pipeline_cross_val(rf_pipeline, X_train, y_train, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_varimp = rf_pipeline.steps[0][1].feature_importances_\n",
    "var_imp_plot(rf_varimp, input_cols, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_c = SVC(gamma='auto', kernel='rbf')\n",
    "svm_pipeline = Pipeline([('scale', scaler), ('clf', svm_c)])\n",
    "pipeline_cross_val(svm_pipeline, X_train, y_train, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_c = GaussianNB()\n",
    "nb_pipeline = Pipeline([('clf', nb_c)])\n",
    "pipeline_cross_val(nb_pipeline, X_train, y_train, print_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
