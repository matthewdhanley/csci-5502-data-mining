{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model, metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Business Data and Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = pd.read_json('../../data/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df.loc[:, 'cat_str_array'] = biz_df.loc[:, 'categories'].str.replace(' ','').str.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bad lat/lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_coords_indices = np.invert(np.array([(v[0]<180) & (v[0]>-180) & (v[1]<90) & (v[1]>-90) \n",
    "                                         for v in biz_df[['longitude','latitude']].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = biz_df.iloc[np.invert(bad_coords_indices)]\n",
    "biz_df.index = range(biz_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las Vegas only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_vegas_misspells = np.array(['Las Vegas', 'Lake Las Vegas', 'Las  Vegas', \n",
    "                               'Las vegas', 'Las Vegass', 'La Vegas', 'Las Vegas,',\n",
    "                               'Las Vegas Nv', 'Las Vegas, NV', 'Las Vegas Nevada', \n",
    "                               'Las Vegas East', 'LasVegas', 'Las Vegas & Henderson',\n",
    "                               'las vegas', 'las Vegas'\n",
    "                              ])\n",
    "biz_df.loc[biz_df['city'].isin(las_vegas_misspells), 'city'] = 'Las Vegas'\n",
    "biz_df = biz_df.loc[biz_df['city'] == 'Las Vegas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse vector for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all possible categories for a restaurant\n",
    "restr_indices = biz_df['categories'].str.contains('Restaurants', na=False)\n",
    "biz_df.loc[:, 'is_restaurant'] = 0\n",
    "biz_df.loc[restr_indices, 'is_restaurant'] = 1\n",
    "restr_cats_all = biz_df.loc[restr_indices, 'cat_str_array']\n",
    "# Count code from: https://stackoverflow.com/questions/51813266/get-unique-values-from-pandas-series-of-lists\n",
    "unique_cats = pd.DataFrame.from_dict(Counter(chain(*restr_cats_all)), orient='index').sort_values(0, ascending=False)\n",
    "# Only include categories that occur more than once, and remove restaurants\n",
    "unique_cats = unique_cats.loc[unique_cats[0] > 1].index[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse restaurant categories vector\n",
    "biz_df.loc[:, 'cat_vector'] = None\n",
    "biz_df.loc[restr_indices, 'cat_vector'] = biz_df.loc[restr_indices, 'cat_str_array'].apply(\n",
    "    lambda x: np.isin(unique_cats, x)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproject lat/lon to NA Albers Equal Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "wgs84_proj = pyproj.Proj(init='epsg:4326')\n",
    "aea_proj = pyproj.Proj('+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs')\n",
    "x_coords, y_coords = pyproj.transform(wgs84_proj, aea_proj, biz_df['longitude'].values, biz_df['latitude'].values)\n",
    "biz_df.loc[:, 'x_coord'], biz_df.loc[:, 'y_coord'] = x_coords, y_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to find the k closest business by lat/lon, faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "dist_tree = neighbors.KDTree([(v[0],v[1]) for v in biz_df[['x_coord','y_coord']].values], leaf_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_bizs(biz_index, df, k, kdtree):\n",
    "    \"\"\"Simple function to retrieve the n closest businesses\"\"\"\n",
    "    indices = kdtree.query(np.array(\n",
    "        [df.loc[biz_index, ['x_coord', 'y_coord']]\n",
    "         .values]), k+1)[1][0, 1:]\n",
    "    return df.iloc[indices]\n",
    "\n",
    "def radius_bizs(biz_index, df, dist, kdtree):\n",
    "    \"\"\"Simple function to retrieve all businesses within distance 'dist' (meters)\"\"\"\n",
    "    indices = kdtree.query_radius(\n",
    "        df.loc[biz_index, ['x_coord', 'y_coord']].values.reshape(1, -1),\n",
    "        r=dist)[0][1:]\n",
    "    if len(indices) > 0:\n",
    "        return df.iloc[indices]\n",
    "    else: \n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc attributes from nearby businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_review_count_feats(review_counts):\n",
    "    return np.mean(review_counts), np.max(review_counts), np.min(review_counts)\n",
    "\n",
    "def calc_star_feats(stars):\n",
    "    return np.mean(stars), np.max(stars), np.min(stars)\n",
    "\n",
    "def nearby_simp(df, kdtree, dist=1000):\n",
    "    new_att_list = [\n",
    "        'nn_count', 'nn_percent_rest', \n",
    "        'nn_all_percent_open', 'nn_rest_percent_open',\n",
    "        'nn_mean_cat_sim', 'nn_max_cat_sim', 'nn_min_cat_sim',\n",
    "        'nn_avg_stars', 'nn_max_stars', 'nn_min_stars', \n",
    "        'nn_avg_review_count', 'nn_max_review_count', 'nn_min_review_count',\n",
    "        'nn_rest_avg_stars', 'nn_rest_max_stars', 'nn_rest_min_stars', \n",
    "        'nn_rest_avg_review_count', 'nn_rest_max_review_count', 'nn_rest_min_review_count',\n",
    "        'nn_weighted_avg_stars','nn_weighted_avg_review_count',\n",
    "        'nn_weighted_sum_stars', 'nn_weighted_sum_review_count']\n",
    "    for att in new_att_list:\n",
    "        df.loc[:,att] = -1\n",
    "    \n",
    "    rest_indices_list = biz_df.loc[biz_df['is_restaurant'] == 1].index\n",
    "    output_list = []\n",
    "    tracker = 1\n",
    "    tcheck = time.time()\n",
    "    for i in rest_indices_list:\n",
    "        if tracker % 100 == 0:\n",
    "            print('100 biz {}'.format(time.time() - tcheck))\n",
    "            tcheck = time.time()\n",
    "            print(tracker)\n",
    "        # Get nearest businesses\n",
    "        nn_df = radius_bizs(i, df, dist, kdtree)\n",
    "        if nn_df.shape[0] > 0:\n",
    "            cur_biz = df.loc[i]\n",
    "            out_dict = {}\n",
    "            \n",
    "            tcheck = time.time()\n",
    "            # Total business counts and open percentage\n",
    "            tot_count = nn_df.shape[0]\n",
    "            out_dict['nn_count'] = tot_count\n",
    "            out_dict['nn_all_percent_open'] = int(100*nn_df['is_open'].sum()/tot_count)\n",
    "            # Ratings\n",
    "            out_dict['nn_avg_stars'], out_dict['nn_max_stars'], out_dict['nn_min_stars'] =\\\n",
    "                   calc_star_feats(nn_df['stars'].values)\n",
    "            \n",
    "            # Review counts\n",
    "            out_dict['nn_avg_review_count'], out_dict['nn_max_review_count'], out_dict['nn_min_review_count'] =\\\n",
    "                   calc_star_feats(nn_df['review_count'].values)\n",
    "            \n",
    "            # Restaurant percentage\n",
    "            nn_rest_df = nn_df.loc[nn_df['categories'].str.contains('Restaurants', na=False)]\n",
    "            rest_count = nn_rest_df.shape[0]\n",
    "            out_dict['nn_percent_rest'] = int(100*rest_count/tot_count)\n",
    "            \n",
    "            # Restaurant specific features, only work if there's a restaurant nearby\n",
    "            if rest_count >0:\n",
    "\n",
    "                out_dict['nn_rest_percent_open'] = int(100*nn_rest_df['is_open'].sum()/rest_count)\n",
    "\n",
    "                # Check restaurant category similarities\n",
    "                dot_products = np.array(\n",
    "                    [np.dot(cur_biz['cat_vector'], cvec) for cvec in nn_rest_df['cat_vector'].values])\n",
    "                out_dict['nn_mean_cat_sim'], out_dict['nn_max_cat_sim'], out_dict['nn_min_cat_sim'] =\\\n",
    "                    np.mean(dot_products), np.max(dot_products), np.min(dot_products)\n",
    "\n",
    "\n",
    "                # Stars and review counts for nearby restaurants\n",
    "                out_dict['nn_rest_avg_stars'], out_dict['nn_rest_max_stars'], out_dict['nn_rest_min_stars'] =\\\n",
    "                       calc_star_feats(nn_rest_df['stars'].values)\n",
    "                out_dict['nn_rest_avg_review_count'], out_dict['nn_rest_max_review_count'], out_dict['nn_rest_min_review_count'] =\\\n",
    "                       calc_star_feats(nn_rest_df['review_count'].values)\n",
    "\n",
    "                # Weighted stars and review counts\n",
    "                dot_prod_total = np.sum(dot_prod_total)\n",
    "                out_dict['nn_weighted_sum_stars'] = np.sum(dot_products*nn_rest_df['stars'])/dot_prod_total\n",
    "                out_dict['nn_weighted_sum_review_count'] = np.sum(dot_products*nn_rest_df['review_count'])/dot_prod_total\n",
    "                out_dict['nn_weighted_avg_stars'] = np.mean(dot_products*nn_rest_df['stars'])/dot_prod_total\n",
    "                out_dict['nn_weighted_avg_review_count'] = np.mean(dot_products*nn_rest_df['review_count'])/dot_prod_total\n",
    "\n",
    "            cur_biz.loc[out_dict.keys()] = tuple(out_dict.values())\n",
    "            output_list.append(cur_biz)\n",
    "            \n",
    "        tracker+=1\n",
    "                   \n",
    "    return pd.DataFrame(output_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_att_df = nearby_atts_simp(biz_df, dist_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_att_df_all = biz_att_df\n",
    "biz_att_df = biz_att_df.loc[biz_att_df.nn_percent_rest>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_att_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of nearby biz atts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KDE hist plot for neighbor counts\n",
    "sns.set_style(style='white')\n",
    "\n",
    "plt.figure(figsize=(20,10), facecolor='white')\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "\n",
    "# Draw the density plot\n",
    "snsfig = sns.distplot(biz_att_df['nn_count'], hist = True, norm_hist=False, kde=False,\n",
    "                      kde_kws = {'linewidth': 3},\n",
    "                      label = 'Total Nearby Businesses',\n",
    "                      bins = range(0, int(np.max(biz_att_df['nn_count'])), 20))\n",
    "snsfig = sns.distplot(biz_att_df['nn_count']*biz_att_df['nn_percent_rest']/100, hist = True,norm_hist=False, kde=False,\n",
    "                      kde_kws = {'linewidth': 3},\n",
    "                      label = 'Nearby Restaurants Only',\n",
    "                      bins = range(0, int(np.max(biz_att_df['nn_count']*biz_att_df['nn_percent_rest']/100)), 20))\n",
    "\n",
    "    \n",
    "# Plot formatting\n",
    "leg = plt.legend(prop={'size': 20}, title = 'User Label')\n",
    "leg.get_title().set_fontsize(25)\n",
    "plt.xlim([0,1000])\n",
    "plt.title('# of Businesses within 1km', size=30)\n",
    "plt.xlabel('# of Nearby Business/Restaurants', size = 20)\n",
    "plt.ylabel('Count', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_bfit(x, y, title, xlabel, ylabel, xlims=None, ylims=None):\n",
    "    plt.figure(figsize=(8,8), facecolor='white')\n",
    "    plt.scatter(x, y, alpha=0.25)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    if xlims != None:\n",
    "        plt.xlim(xlims[0], xlims[1])\n",
    "    else:\n",
    "        xlims = (np.min(x), np.max(x))\n",
    "    if ylims != None:\n",
    "        plt.ylim(ylims[0], ylims[1])\n",
    "    \n",
    "    plt_x = np.arange(xlims[0], xlims[1], (xlims[1]-xlims[0])/len(x))\n",
    "    print(plt_x)\n",
    "    plt.plot(plt_x, m*plt_x + b, '--', linewidth=2.5, color='orange')\n",
    "    \n",
    "    # Labels\n",
    "    plt.title(title, size=20)\n",
    "    plt.xlabel(xlabel, size=16)\n",
    "    plt.ylabel(ylabel, size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_avg_stars, biz_att_df.stars, 'Neighbors Average Rating vs. Restaurant Rating',\n",
    "            'Neighbors\\' Avg Rating', 'Rating', ylims=[0.8,5.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_avg_review_count, biz_att_df.stars, 'Neighbor Avg Review Count vs Rating',\n",
    "            'Neighbor Avg Review Count', 'Rating', ylims=[0.8,5.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_avg_review_count, biz_att_df.review_count, 'Neighbor Avg Review Count vs Review Count',\n",
    "            'Neighbor Avg Review Count', 'Restaurant Review Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_weighted_sum_stars, biz_att_df.stars, 'Similarity-Weighted Sum of Neighbor Ratings',\n",
    "            'Weighted Neighbor Stars', 'Rating', ylims=[0.8,5.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_weighted_sum_review_count, biz_att_df.stars, 'Similarity-Weighted Sum of Neighbor Review Counts',\n",
    "            'Weighted Neighbor Review Counts', 'Rating', ylims=[0.8,5.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_bfit(biz_att_df.nn_mean_cat_sim, biz_att_df.stars, 'Mean Neighbor Restaurant Category Similarity',\n",
    "            'Mean Neighbor Similarity', 'Rating', xlims=[-0.2,5],ylims=[0.8,5.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import ogr, osr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "# Get county based on lat lon\n",
    "counties = gpd.read_file('../../data/census/census_shapefiles/tl_2017_us_county.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(biz_att_df.longitude, biz_att_df.latitude)]\n",
    "crs = {'init': 'epsg:4326'} #http://www.spatialreference.org/ref/epsg/2263/\n",
    "geo_df = gpd.GeoDataFrame(biz_att_df, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get county of each.\n",
    "county_intersection = [geo_df.geometry.intersects(cg) for cg in counties.geometry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They're all in the same county, but at least it's Clark County (the right one)\n",
    "np.unique(np.where(np.array(county_intersection)!=0)[0])\n",
    "counties.loc[151]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
